{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Pipeline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9844448e7b14172a5d493aecaf04eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8186f446651f47c483582a43db87d2b1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_30de61f8f9744fc99f890e9db25d93c1",
              "IPY_MODEL_b8b7078cd261459b8e929b4ad30a130e"
            ]
          }
        },
        "8186f446651f47c483582a43db87d2b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "30de61f8f9744fc99f890e9db25d93c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_be0112c4d61a49e08994aa63dbb48b56",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 337926286,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 337926286,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dae9184ee3c54f0d9d599f438806d612"
          }
        },
        "b8b7078cd261459b8e929b4ad30a130e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e3a3c6a355841989f321bbc70a847a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 322M/322M [00:01&lt;00:00, 215MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7929bf7a08c48f28207f0dbe2cff9f6"
          }
        },
        "be0112c4d61a49e08994aa63dbb48b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dae9184ee3c54f0d9d599f438806d612": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e3a3c6a355841989f321bbc70a847a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7929bf7a08c48f28207f0dbe2cff9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB4NbmoX3i65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install torchaudio soundfile tensorboard_logger\n",
        "! sudo apt-get install ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_1EfIn83sH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torchaudio\n",
        "import torch.autograd as autograd\n",
        "import torch.utils.data as data\n",
        "from torch.nn import CTCLoss\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import tqdm\n",
        "import soundfile\n",
        "import tensorboard_logger as tb_log\n",
        "import librosa \n",
        "import IPython\n",
        "\n",
        "import wave\n",
        "from math import sin, cos, pi, log, exp, floor, ceil\n",
        "from torch.nn import BCELoss, BCEWithLogitsLoss , CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJE2rw2j0TjW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "__Набор данных для обучения и теста__ состоит из позитивных примеров-аудиодорожек(записано произношение \"Alexa\") и негативных примеров (любой другой звук/речь/шум).\n",
        "* Сбор и аугментация позитивных примеров: запись произношения \"Alexa\" одним человеком в четырёх различных вариациях(две на train, две на test), дублирование этого произношения 160(train), 80(test); изменение скорости (x0.85, x1.15) и добавление шумов (0.01, 0.02, 0.03) - 200 train, 100 test\n",
        "\n",
        "* Негативные примеры: датасет LibriSpeech (dev-clean) ~(80/20)\n",
        "\n",
        "\n",
        "\n",
        "Load LibriSpeech dataset as negative speech examples (speak anything but ”Alexa”)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRbtc9WL3usi",
        "colab_type": "text"
      },
      "source": [
        "## Загрузка и предобработка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn-Ut4Pk3tap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "a9844448e7b14172a5d493aecaf04eef",
            "8186f446651f47c483582a43db87d2b1",
            "30de61f8f9744fc99f890e9db25d93c1",
            "b8b7078cd261459b8e929b4ad30a130e",
            "be0112c4d61a49e08994aa63dbb48b56",
            "dae9184ee3c54f0d9d599f438806d612",
            "2e3a3c6a355841989f321bbc70a847a2",
            "c7929bf7a08c48f28207f0dbe2cff9f6"
          ]
        },
        "outputId": "69c45b35-a43b-4d54-87dd-618b9dab36cc"
      },
      "source": [
        "dataset_url = 'dev-clean'\n",
        "libri_data = torchaudio.datasets.LIBRISPEECH('./', url=dataset_url, folder_in_archive='LibriSpeech', download=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9844448e7b14172a5d493aecaf04eef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=337926286.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0kzSf1Z30Fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/LibriSpeech/')\n",
        "# Create small datasets\n",
        "! mkdir 'aug_wavs' 'train' 'test'\n",
        "os.mkdir('/content/LibriSpeech/train/1')\n",
        "os.mkdir('/content/LibriSpeech/train/0')\n",
        "os.mkdir('/content/LibriSpeech/test/1')\n",
        "os.mkdir('/content/LibriSpeech/test/0')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ANAUqMzSxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rm -r /content/LibriSpeech"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfCTcbWu33mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Beta test\n",
        "\n",
        "! find /content/LibriSpeech/dev-clean/2078 -type f -name '*.flac' -exec mv -i {} /content/LibriSpeech/test/0/  \\;\n",
        "! rm -r /content/LibriSpeech/dev-clean/2078\n",
        "! find /content/LibriSpeech/dev-clean/2086 -type f -name '*.flac' -exec mv -i {} /content/LibriSpeech/test/0/  \\;\n",
        "! rm -r /content/LibriSpeech/dev-clean/2086\n",
        "! find /content/LibriSpeech/dev-clean/2277 -type f -name '*.flac' -exec mv -i {} /content/LibriSpeech/test/0/  \\;\n",
        "! rm -r /content/LibriSpeech/dev-clean/2277\n",
        "! find /content/LibriSpeech/dev-clean/2412 -type f -name '*.flac' -exec mv -i {} /content/LibriSpeech/test/0/  \\;\n",
        "! rm -r /content/LibriSpeech/dev-clean/2412\n",
        "! find /content/LibriSpeech/dev-clean/2428 -type f -name '*.flac' -exec mv -i {} /content/LibriSpeech/test/0/  \\;\n",
        "! rm -r /content/LibriSpeech/dev-clean/2428\n",
        "! find /content/LibriSpeech/dev-clean/251 -type f -name '*.flac' -exec mv -i {} /content/LibriSpeech/test/0/  \\;\n",
        "! rm -r /content/LibriSpeech/dev-clean/251\n",
        "\n",
        "# Beta train\n",
        "# Move all files in subfolders to another directory without structure\n",
        "! find /content/LibriSpeech/dev-clean -type f -print0 | xargs -0 mv -t /content/LibriSpeech/train/0\n",
        "\n",
        "! rm -r /content/LibriSpeech/dev-clean/"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgyJFrkqvf-C",
        "colab_type": "text"
      },
      "source": [
        "## Конвертируем загруженные аудио в .wav формат с помощью shell scrpit \n",
        "Load `flac_to_wav.sh`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tcrB67835XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! chmod +x '/content/LibriSpeech/flac_to_wav.sh' # giving a permision"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id2xMPz4364z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/LibriSpeech/')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2LGsG7l4AWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! '/content/LibriSpeech/flac_to_wav.sh'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGDfyq32sarB",
        "colab_type": "text"
      },
      "source": [
        "## Зугружаем .wav c произношением \"Alexa\" для генерации позитивных примеров\n",
        " Load custom wav"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlOrjHTxAGAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik8LdCLJxnP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# C помощью следующих функций аугментируем позитивный .wav \n",
        "\n",
        "def add_noise(wav, noise_factor=0.02):\n",
        "    noise = np.random.normal(size=len(wav))\n",
        "    noisy_wav = wav + noise_factor * noise\n",
        "    noisy_wav = noisy_wav.astype(type(wav[0])) # Set initial dtype\n",
        "    return noisy_wav\n",
        "\n",
        "def speed_change(wav, speed_factor=1.5):\n",
        "    y_speed = wav.copy()\n",
        "    speed_change = speed_factor\n",
        "    tmp = librosa.effects.time_stretch(y_speed.astype('float64'), speed_change)\n",
        "    minlen = min(y_speed.shape[0], tmp.shape[0])\n",
        "    y_speed *= 0 \n",
        "    y_speed[0:minlen] = tmp[0:minlen]\n",
        "    return y_speed\n",
        "    # return librosa.effects.time_stretch(data, speed_factor)\n",
        "\n",
        "def dummy_data_aug(wav_paths: list, dest_folder, n=10):\n",
        "    counter = 0 \n",
        "    for _ in range(n):\n",
        "        for i in wav_paths:\n",
        "            y, sr = librosa.load(i)\n",
        "            y1 = speed_change(y, 1.15)\n",
        "            librosa.output.write_wav(dest_folder+'%i.wav' % counter, y1, sr)\n",
        "            counter+=1\n",
        "            y2 = speed_change(y, 0.85)\n",
        "            librosa.output.write_wav(dest_folder+'%i.wav' % counter, y2, sr)\n",
        "            counter+=1\n",
        "            noisy_y1 = add_noise(y, 0.01)\n",
        "            librosa.output.write_wav(dest_folder+'%i.wav' % counter, noisy_y1, sr)\n",
        "            counter+=1\n",
        "            noisy_y2 = add_noise(y, 0.02)\n",
        "            librosa.output.write_wav(dest_folder+'%i.wav' % counter, noisy_y2, sr)\n",
        "            counter+=1\n",
        "            noisy_y3 = add_noise(y, 0.03)\n",
        "            librosa.output.write_wav(dest_folder+'%i.wav' % counter, noisy_y3, sr)\n",
        "            counter+=1\n",
        "            librosa.output.write_wav(dest_folder+'%i.wav' % counter, y, sr)\n",
        "            counter+=1\n",
        "            librosa.output.write_wav(dest_folder+'%i.wav' % counter, y, sr)\n",
        "            counter+=1\n",
        "            librosa.output.write_wav(dest_folder+'%i.wav' % counter, y, sr)\n",
        "            counter+=1\n",
        "            librosa.output.write_wav(dest_folder+'%i.wav' % counter, y, sr)\n",
        "            counter+=1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uI9F1F70oyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Positive exapmles to train folder\n",
        "dummy_data_aug(['/content/Alexa1.wav', '/content/Alexa3.wav'], '/content/LibriSpeech/train/1/', n=20)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RJqWLrNwPwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Positive exapmles to test folder\n",
        "dummy_data_aug(['/content/Alexa2.wav', '/content/Alexa4.wav'], '/content/LibriSpeech/test/1/', n=10)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZXvPcMb5o1k",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XIRdEMc5noH",
        "colab_type": "text"
      },
      "source": [
        "## Создадим загрузчик данных с извлечением признаков(spectrogram) из .wav \n",
        " Source https://github.com/adiyoss/GCommandsPytorch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ86ph-WfX_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUDIO_EXTENSIONS = [\n",
        "    '.wav', '.WAV',\n",
        "]\n",
        "\n",
        "\n",
        "def is_audio_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in AUDIO_EXTENSIONS)\n",
        "\n",
        "\n",
        "def find_classes(dir):\n",
        "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "    classes.sort()\n",
        "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "\n",
        "def make_dataset(dir, class_to_idx):\n",
        "    spects = []\n",
        "    dir = os.path.expanduser(dir)\n",
        "    for target in sorted(os.listdir(dir)):\n",
        "        d = os.path.join(dir, target)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "\n",
        "        for root, _, fnames in sorted(os.walk(d)):\n",
        "            for fname in sorted(fnames):\n",
        "                if is_audio_file(fname):\n",
        "                    path = os.path.join(root, fname)\n",
        "                    item = (path, class_to_idx[target])\n",
        "                    spects.append(item)\n",
        "    return spects\n",
        "\n",
        "\n",
        "def spect_loader(path, window_size, window_stride, window, normalize, max_len=101):\n",
        "    y, sr = librosa.load(path, sr=16000)\n",
        "    # n_fft = 4096\n",
        "    n_fft = int(sr * window_size)\n",
        "    win_length = n_fft\n",
        "    hop_length = int(sr * window_stride)\n",
        "\n",
        "    # # STFT\n",
        "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
        "                     win_length=win_length, window=window)\n",
        "    \n",
        "    # Compute spectrogram feature\n",
        "    spect, phase = librosa.magphase(D)\n",
        "\n",
        "\n",
        "    # S = log(S+1)\n",
        "    spect = np.log1p(spect) #   \n",
        "\n",
        "    # spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft) # 128 amplitudes\n",
        "    \n",
        "\n",
        "    # make all spects with the same dims\n",
        "    if spect.shape[1] < max_len:\n",
        "        pad = np.zeros((spect.shape[0], max_len - spect.shape[1]))\n",
        "        spect = np.hstack((spect, pad))\n",
        "    elif spect.shape[1] > max_len:\n",
        "        spect = spect[:, :max_len]\n",
        "    spect = np.resize(spect, (1, spect.shape[0], spect.shape[1]))\n",
        "    spect = torch.FloatTensor(spect)\n",
        "\n",
        "    # z-score normalization\n",
        "    if normalize:\n",
        "        mean = spect.mean()\n",
        "        std = spect.std()\n",
        "        if std != 0:\n",
        "            spect.add_(-mean)\n",
        "            spect.div_(std)\n",
        "\n",
        "    return spect\n",
        "\n",
        "\n",
        "class DatasetLoader(data.Dataset):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        root (string): Root directory path.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        window_size: window size for the stft, default value is .02\n",
        "        window_stride: window stride for the stft, default value is .01\n",
        "        window_type: typye of window to extract the stft, default value is 'hamming'\n",
        "        normalize: boolean, whether or not to normalize the spect to have zero mean and one std\n",
        "        max_len: the maximum length of frames to use\n",
        "     Attributes:\n",
        "        classes (list): List of the class names.\n",
        "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
        "        spects (list): List of (spects path, class_index) tuples\n",
        "        STFT parameter: window_size, window_stride, window_type, normalize\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None, target_transform=None, window_size=.02,\n",
        "                 window_stride=.01, window_type='hann', normalize=True, max_len=101):\n",
        "        classes, class_to_idx = find_classes(root)\n",
        "        spects = make_dataset(root, class_to_idx)\n",
        "        if len(spects) == 0:\n",
        "            raise (RuntimeError(\"Found 0 sound files in subfolders of: \" + root + \"Supported audio file extensions are: \" + \",\".join(AUDIO_EXTENSIONS)))\n",
        "\n",
        "        self.root = root\n",
        "        self.spects = spects\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.loader = spect_loader\n",
        "        self.window_size = window_size\n",
        "        self.window_stride = window_stride\n",
        "        self.window_type = window_type\n",
        "        self.normalize = normalize\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (spect, target) where target is class_index of the target class.\n",
        "        \"\"\"\n",
        "        path, target = self.spects[index]\n",
        "          \n",
        "        spect = self.loader(path, self.window_size, self.window_stride, self.window_type, self.normalize, self.max_len)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            spect = self.transform(spect)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return spect, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spects)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93si1_uw6M4V",
        "colab_type": "text"
      },
      "source": [
        "## Эмуляция Wake-Word модели на основе highway блоков\n",
        "![_здесь должна быть картинка архитектуры модели_](https://github.com/eestien/VAD-Attack/blob/master/WWAlexa.png)\n",
        "\n",
        " papers __Adversarial Music: Real World Audio Adversary\n",
        "Against Wake-word Detection System__ _(http://papers.nips.cc/paper/9362-adversarial-music-real-world-audio-adversary-against-wake-word-detection-system.pdf)_, __Adversarial Music: Real world Audio Adversary against Wake-word Detection\n",
        "Systems\n",
        "10-708A S19 Final Report__ _(https://sailinglab.github.io/pgm-spring-2019/assets/project/final-reports/project29.pdf)_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRRU_R244RF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Highway(nn.Module):\n",
        "    def __init__(self, size, num_layers, f):\n",
        "\n",
        "        super(Highway, self).__init__()\n",
        "\n",
        "        # gen features from wav\n",
        "        # self.spec_layer = Spectrogram.STFT(n_fft=2048, freq_bins=None, hop_length=512, window='hann', freq_scale='no', center=True, pad_mode='reflect', fmin=50,fmax=6000, sr=16000, trainable=False, output_format='Magnitude', device='cuda')\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
        "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
        "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            :param x: tensor with shape of [batch_size, size]\n",
        "            :return: tensor with shape of [batch_size, size]\n",
        "            applies σ(x) ⨀ (f(G(x))) + (1 - σ(x)) ⨀ (Q(x)) transformation | G and Q is affine transformation,\n",
        "            f is non-linear transformation, σ(x) is affine transformation with sigmoid non-linearition\n",
        "            and ⨀ is element-wise multiplication\n",
        "            \"\"\"\n",
        "        # x = self.spec_layer(x)\n",
        "\n",
        "        for layer in range(self.num_layers):\n",
        "            gate = torch.sigmoid(self.gate[layer](x))\n",
        "\n",
        "            nonlinear = self.f(self.nonlinear[layer](x))\n",
        "            linear = self.linear[layer](x)\n",
        "\n",
        "            x = gate * nonlinear + (1 - gate) * linear\n",
        "\n",
        "        return x\n",
        "\n",
        "# For concatenating features from adjacent frames\n",
        "def add_context(X_raw, k):\n",
        "    X_context = []\n",
        "    for utterance in X_raw:\n",
        "        n, d = utterance.shape # amplitude values, bottleneck size\n",
        "        utterance_with_context = torch.zeros((n, (2*k+1)*d)).cuda()\n",
        "        for j in np.arange(-k, k+1, 1):\n",
        "            # print(j)\n",
        "            if j<0:\n",
        "                utterance_with_context[:j,d*(j+k):d*(j+k+1)] = utterance[-j:]\n",
        "            elif j==0:\n",
        "                utterance_with_context[:,d*(j+k):d*(j+k+1)] = utterance\n",
        "            else:\n",
        "                utterance_with_context[j:,d*(j+k):d*(j+k+1)] = utterance[:-j]\n",
        "        X_context.append(utterance_with_context.unsqueeze(0))\n",
        "    return torch.cat(X_context)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPnP_ppy4S8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input size: N_batch x T x Dim.\n",
        "class myAlexa(nn.Module):\n",
        "    def __init__(self, n_input, n_bottleneck = 32, n_output= 1, context = 5):\n",
        "        super(myAlexa, self).__init__()\n",
        "        self.encoder = Highway(n_input, 4, f=torch.nn.functional.relu)\n",
        "        self.bottleneck = nn.Linear(n_input, n_bottleneck)\n",
        "        self.classifier = Highway(n_bottleneck*(2*context+1), 6, f=torch.nn.functional.relu)\n",
        "        self.linear = nn.Linear(n_bottleneck*(2*context+1), n_output)\n",
        "        self.context = context\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "       \n",
        "    def forward(self, x): # x: N_batch x T x Dim.\n",
        "        # x = torch.FloatTensor(x)\n",
        "        out = self.encoder(x) # N_batch x T x Dim.\n",
        "        out = self.bottleneck(out) #  N_batch x T x 32\n",
        "        out = add_context(out, self.context) # N_batch x T x (32*11)\n",
        "        out = self.classifier(out) # N x T x (32*11)\n",
        "        out = self.linear(out) # N x T x 1\n",
        "        # out = self.sigmoid(out)\n",
        "        return out # torch.sigmoid(out) # torch.nn.functional.softmax(out, dim=2)# out # torch.FloatTensor(out.cpu()).cuda() # torch.nn.functional.softmax(out, dim=2)\n",
        "        \n",
        "\n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6mtCaHR4VIN",
        "colab_type": "text"
      },
      "source": [
        "## Karplus-Strong pytorch nn.Module \n",
        "### Генерация гитарной музыкис помощью Karplus-Strong алгоритма\n",
        "\n",
        "За основную мелодию возьмём набор из нескольких случайных нот.\n",
        "Зададим параметры, которые будем оптимизировать `boost` и `freq`\n",
        "\n",
        "\n",
        "\n",
        "  algorithm source https://github.com/mdoege/PySynth/blob/master/demosongs.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBdSf3PA4h2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wave, struct\n",
        "import numpy as np\n",
        "from math import sin, cos, pi, log, exp, floor, ceil\n",
        "\n",
        "\n",
        "## boost & freq init values\n",
        "boost = 1.1\n",
        "freq = 32\n",
        "bpm = 126\n",
        "transpose = 0\n",
        "\n",
        "\n",
        "song4_rh = (\n",
        "  ('e', 8), ('f#', 8),\n",
        "  ('g*', 4), ('f#', 8), ('e', 8), ('d#*', 4), ('e', 8), ('f#', 8),\n",
        "  ('b3*', 4), ('c#', 8), ('d#', 8), ('e*', 4), ('d', 8), ('c', 8),\n",
        "  ('b3*', 4), ('a3', 8), ('g3', 8), ('f#3*', 4), ('g3', 8), ('a3', 8),\n",
        "  ('b3*', 8), ('a3', 8), ('g3', 8), ('f#3', 8), ('e3*', 4), ('e', 8), ('f#', 8),\n",
        "  ('g*', 4), ('f#', 8), ('e', 8), ('d#*', 4), ('e', 8), ('f#', 8),\n",
        "  ('b3*', 4), ('c#', 8), ('d#', 8), ('e*', 4), ('d', 8), ('c', 8),\n",
        "  ('b3*', 4), ('a3', 8), ('g3', 8), ('g3*', 32), ('f#3*', 32), ('g3*', 32), ('f#3*', 32), ('g3*', 32), ('f#3*', 32), ('g3*', 32), ('f#3*', 6.4), ('g3', 8), ('g3*', -2),\n",
        ")\n",
        "\n",
        "pitchhz, keynum = {}, {}\n",
        "\n",
        "keys_s = ('a', 'a#', 'b',  'c',  'c#', 'd', 'd#', 'e',  'f',  'f#', 'g', 'g#')\n",
        "keys_f = ('a', 'bb', 'b',  'c',  'db', 'd', 'eb', 'e',  'f',  'gb', 'g', 'ab')\n",
        "keys_e = ('a', 'bb', 'cb', 'b#', 'db', 'd', 'eb', 'fb', 'e#', 'gb', 'g', 'ab')\n",
        "\n",
        "def getfreq(pr = False):\n",
        "\tif pr:\n",
        "\t\tprint(\"Piano key frequencies (for equal temperament):\")\n",
        "\t\tprint(\"Key number\\tScientific name\\tFrequency (Hz)\")\n",
        "\tfor k in range(88):\n",
        "\t\tfreq = 27.5 * 2.**(k/12.)\n",
        "\t\toct = (k + 9) // 12\n",
        "\t\tnote = '%s%u' % (keys_s[k%12], oct)\n",
        "\t\tif pr:\n",
        "\t\t\tprint(\"%10u\\t%15s\\t%14.2f\" % (k+1, note.upper(), freq))\n",
        "\t\tpitchhz[note] = freq\n",
        "\t\tkeynum[note] = k\n",
        "\t\tnote = '%s%u' % (keys_f[k%12], oct)\n",
        "\t\tpitchhz[note] = freq\n",
        "\t\tkeynum[note] = k\n",
        "\t\tnote = '%s%u' % (keys_e[k%12], oct)\n",
        "\t\tpitchhz[note] = freq\n",
        "\t\tkeynum[note] = k\n",
        "\treturn pitchhz, keynum\n",
        "\n",
        "\n",
        "class AudioGenKS(nn.Module):\n",
        "    def __init__(self, bpm=bpm, transpose=transpose, pause=0, freq = freq, loud=1, boost=boost,repeat=0,fn=\"out.wav\",silent=False):\n",
        "        super(AudioGenKS, self).__init__()\n",
        "        self.bpm = nn.Parameter(torch.autograd.Variable(torch.tensor(bpm, dtype=torch.float)))\n",
        "        self.transpose = nn.Parameter(torch.autograd.Variable(torch.tensor(transpose, dtype=torch.float)))\n",
        "        self.pause = pause \n",
        "        self.freq = nn.Parameter(torch.autograd.Variable(torch.tensor(freq, dtype=torch.float))) # torch.nn.Parameter(torch.tensor(freq))    \n",
        "        self.loud = loud # nn.Parameter(torch.autograd.Variable(torch.tensor(loud, dtype=torch.float)))\n",
        "        self.fn = fn\n",
        "        self.boost = nn.Parameter(torch.autograd.Variable(torch.tensor(boost, dtype=torch.float)))\n",
        "        self.repeat = repeat\n",
        "\n",
        "    def forward(self,song):\n",
        "\n",
        "        f=wave.open(self.fn,'w')\n",
        "\n",
        "        f.setnchannels(1)\n",
        "        f.setsampwidth(2)\n",
        "        f.setframerate(44100)\n",
        "        f.setcomptype('NONE','Not Compressed')\n",
        "\n",
        "        bpmfac = 120./self.bpm\n",
        "        \n",
        "        \n",
        "        pitchhz, keynum = {}, {}\n",
        "\n",
        "        def getfreq(pr = False):\n",
        "          if pr:\n",
        "            print(\"Piano key frequencies (for equal temperament):\")\n",
        "            print(\"Key number\\tScientific name\\tFrequency (Hz)\")\n",
        "          for k in range(88):\n",
        "            freq = 27.5 * 2.**(k/12.)\n",
        "            oct = (k + 9) // 12\n",
        "            note = '%s%u' % (keys_s[k%12], oct)\n",
        "            if pr:\n",
        "              print(\"%10u\\t%15s\\t%14.2f\" % (k+1, note.upper(), freq))\n",
        "            pitchhz[note] = freq\n",
        "            keynum[note] = k\n",
        "            note = '%s%u' % (keys_f[k%12], oct)\n",
        "            pitchhz[note] = freq\n",
        "            keynum[note] = k\n",
        "            note = '%s%u' % (keys_e[k%12], oct)\n",
        "            pitchhz[note] = freq\n",
        "            keynum[note] = k\n",
        "          return pitchhz, keynum\n",
        "\n",
        "        pitchhz, keynum = getfreq()\n",
        "\n",
        "        def linint(arr, x):\n",
        "          \"Interpolate an (X, Y) array linearly.\"\n",
        "          for v in arr:\n",
        "            if v[0] == x: return v[1]\n",
        "          xvals = [v[0] for v in arr]\n",
        "          ux = max(xvals)\n",
        "          lx = min(xvals)\n",
        "          try: assert lx <= x <= ux\n",
        "          except:\n",
        "            #print lx, x, ux\n",
        "            raise\n",
        "          for v in arr:\n",
        "            if v[0] > x and v[0] - x <= ux - x:\n",
        "              ux = v[0]\n",
        "              uy = v[1]\n",
        "            if v[0] < x and x - v[0] >= lx - x:\n",
        "              lx = v[0]\n",
        "              ly = v[1]\t\t\n",
        "          #print lx, ly, ux, uy\n",
        "          return (float(x) - lx) / (ux - lx) * (uy - ly) + ly\n",
        "\n",
        "        def length(l):\n",
        "            return 88200./l*bpmfac\n",
        "\n",
        "        def waves2(hz,l):\n",
        "            a=44100./hz\n",
        "            b=float(l)/44100.*hz\n",
        "            return [a,round(b.item())]\n",
        "\n",
        "        def asin(x):\n",
        "            return sin(2.*pi*x)\n",
        "\n",
        "        def render2(a, b, vol, pos, knum, note, endamp = .25, sm = 10):\n",
        "          b2 = (1. - self.pause) * b\n",
        "          l=waves2(a, b2)\n",
        "          ow=b''\n",
        "          q=int(l[0]*l[1])\n",
        "\n",
        "          lf = log(a)\n",
        "          t = (lf-3.) / (8.5-3.)\n",
        "          volfac = 1. + .8 * t * cos(pi/5.3*(lf-3.))\n",
        "          snd_len = int((10.-lf)*q)\n",
        "          if lf < 4: snd_len *= 2\n",
        "          x = np.arange(snd_len)\n",
        "          s = x / float(q)\n",
        "\n",
        "          ls = np.log(1. + s)\n",
        "          kp_len = int(l[0])\n",
        "          kps1 = np.zeros(snd_len)\n",
        "          kps2 = np.zeros(snd_len)\n",
        "          kps1[:kp_len] = np.random.normal(size = kp_len)\n",
        "\n",
        "          for t in range(kp_len):\n",
        "            kps2[t] = kps1[t:t+sm].mean()\n",
        "          delt = float(l[0])\n",
        "          li = int(floor(delt))\n",
        "          hi = int(ceil(delt))\n",
        "          ifac = delt % 1\n",
        "          delt2 = delt * (floor(delt) - 1) / floor(delt)\n",
        "          ifac2 = delt2 % 1\n",
        "          falloff = (4./lf*endamp)**(1./l[1])\n",
        "          for t in range(hi, snd_len):\n",
        "            v1 = ifac * kps2[t-hi]   + (1.-ifac) * kps2[t-li]\n",
        "            v2 = ifac2 * kps2[t-hi+1] + (1.-ifac2) * kps2[t-li+1]\n",
        "            kps2[t] += .5 * (v1 + v2) * falloff\n",
        "          data[pos:pos+snd_len] += kps2*vol*volfac\n",
        "\n",
        "        ex_pos = 0.\n",
        "        t_len = 0\n",
        "        for y, x in song:\n",
        "          if x < 0:\n",
        "            t_len+=length(-2.*x/3.)\n",
        "          else:\n",
        "            t_len+=length(x)\n",
        "\n",
        "        # Empty music\n",
        "        # data = np.zeros(int((self.repeat+1)*t_len + 20 * 44100))\n",
        "        data = torch.autograd.Variable(torch.zeros(int((self.repeat+1)*t_len + 20 * 44100)))\n",
        "        #print len(data)/44100., \"s allocated\"\n",
        "\n",
        "        for rp in range(self.repeat+1):\n",
        "          for nn, x in enumerate(song):\n",
        "            if not nn % 4:\n",
        "              print(\"[%u/%u]\\t\" % (nn+1,len(song)))\n",
        "            if x[0]!='r':\n",
        "              if x[0][-1] == '*':\n",
        "                vol = self.boost.item()\n",
        "                note = x[0][:-1]\n",
        "              else:\n",
        "                vol = 1.\n",
        "                note = x[0]\n",
        "              if not note[-1].isdigit():\n",
        "                note += '4'\t\t# default to fourth octave\n",
        "              a=pitchhz[note]\n",
        "              kn = keynum[note]\n",
        "              a = a * 2**self.transpose\n",
        "              if x[1] < 0:\n",
        "                b=length(-2.*x[1]/3.)\n",
        "              else:\n",
        "                b=length(x[1])\n",
        "\n",
        "              render2(a, b, vol, int(ex_pos), kn, note)\n",
        "              ex_pos = ex_pos + b\n",
        "\n",
        "            if x[0]=='r':\n",
        "              b=length(x[1])\n",
        "              ex_pos = ex_pos + b\n",
        "\n",
        "        ##########################################################################\n",
        "        # Write to output file (in WAV format)\n",
        "        ##########################################################################\n",
        "        \n",
        "\n",
        "        data = data / (data.max() * 2.)\n",
        "        out_len = int(2. * 44100. + ex_pos+.5)\n",
        "        data2 = np.zeros(out_len, np.short)\n",
        "        data2[:] = 32000. * data[:out_len]\n",
        "        f.writeframes(data2.tostring())\n",
        "        f.close()\n",
        "        \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FibPtji64arf",
        "colab_type": "text"
      },
      "source": [
        "## 1. Обучение Wake-word модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk0JhcENqtkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model\n",
        "model = myAlexa(161) # 161 feature values for each window\n",
        "\n",
        "# Move model to cuda\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Create train and test dataloaders\n",
        "train_dataset = DatasetLoader('/content/LibriSpeech/train/', window_size=0.02, window_stride=0.01,\n",
        "                               window_type='hamming', normalize=True, max_len=101)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, sampler=None)\n",
        "\n",
        "test_dataset = DatasetLoader('/content/LibriSpeech/test/', window_size=0.02, window_stride=0.01,\n",
        "                               window_type='hamming', normalize=True, max_len=101)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, sampler=None)\n",
        "\n",
        "\n",
        "# Define train function\n",
        "def train(loader, model, optimizer, epoch, cuda, log_interval, verbose=True):\n",
        "    model.train()\n",
        " \n",
        "    global_epoch_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "\n",
        "        # data = torch.add(data, bach) # train sample + generated sample  # THREAT MODEL\n",
        "\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        try: \n",
        "            data = data.reshape(batch_size, -1, 161)\n",
        "            \n",
        "            data, target = Variable(data), Variable(target)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)# .squeeze(1)\n",
        "            \n",
        "            loss = criterion(output, target.unsqueeze(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            global_epoch_loss += loss.item()\n",
        "            if verbose:\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                            epoch, batch_idx * len(data), len(loader.dataset), 100.\n",
        "                            * batch_idx / len(loader), loss.item()))\n",
        "        except Exception:\n",
        "            print('E')\n",
        "            pass\n",
        "    return global_epoch_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def test(loader, model, cuda, verbose=True):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        with torch.no_grad():\n",
        "            try: \n",
        "                data = data.reshape(batch_size, -1, 161)\n",
        "            \n",
        "                output = model(data)\n",
        "                test_loss += criterion(output , target.unsqueeze(-1)).item()\n",
        "                # test_loss += torch.nn.functional.nll_loss(output, target.unsqueeze(-1)).item()  # sum up batch loss\n",
        "                pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    test_loss /= len(loader.dataset)\n",
        "    if verbose:\n",
        "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(loader.dataset), 100. * correct / len(loader.dataset)))\n",
        "    return test_loss\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBucFAHauRBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_train(n_epochs):\n",
        "    best_valid_loss = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_loss_train = train(train_loader, model, optimizer, epoch, cuda=True, log_interval=25, verbose=True)\n",
        "        epoch_loss_valid = test(test_loader, model, cuda=True)\n",
        "        \n",
        "        epoch += 1"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z42Zw8Zlq7B-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "592d531c-4742-41a4-9670-c210d14137df"
      },
      "source": [
        "# Run model training\n",
        "assert torch.cuda.is_available()\n",
        "run_train(n_epochs=1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/2641 (0%)]\tLoss: 4.615066\n",
            "Train Epoch: 0 [800/2641 (30%)]\tLoss: 0.000372\n",
            "Train Epoch: 0 [1600/2641 (60%)]\tLoss: 0.000274\n",
            "Train Epoch: 0 [2400/2641 (90%)]\tLoss: 0.001860\n",
            "E\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 574/602 (95%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVJPdD4jE9Ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, '/content/AlexaModel.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ORtkEUxxtdD",
        "colab_type": "text"
      },
      "source": [
        "### Генерируем базовую струнную мелодию `bach_audio11.wav` с помощью алгоритма Карплуса-Стронга без оптимизированных для атаки параметров. Проверяем насколько эта мелодия понизила точночть wake-word модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmlthO6HyPUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AG = AudioGenKS(fn='/content/bach_audio11.wav', bpm=126, transpose=1, freq=32, loud=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SsZtN3FzsRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "song4_ = (\n",
        "  ('e', 6), ('f#', 6), ('e', 6), ('f#', 6), ('e', 6), ('f#', 6), ('e', 6), ('f#', 6))\n",
        "AG(song4_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXX4XEKesXB4",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Проверяем точность модели с фоновой музыкой , сгенрерированной без оптимизированных параметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "698sH28Rs5NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_plus_ks_init(loader, model, cuda, verbose=True):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    bach = spect_loader('/content/bach_audio11.wav', 0.02, 0.01, 'hann', True, max_len=101)\n",
        "    for data, target in loader:\n",
        "\n",
        "        bach = bach.expand(batch_size,161,101)\n",
        "        data = torch.add(data, bach) # train sample + generated sample  # \n",
        "\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        with torch.no_grad():\n",
        "            try: \n",
        "                data = data.reshape(batch_size, -1, 161)\n",
        "            \n",
        "                output = model(data)\n",
        "                test_loss += criterion(output , target.unsqueeze(-1)).item()\n",
        "                # test_loss += torch.nn.functional.nll_loss(output, target.unsqueeze(-1)).item()  # sum up batch loss\n",
        "                pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    test_loss /= len(loader.dataset)\n",
        "    if verbose:\n",
        "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(loader.dataset), 100. * correct / len(loader.dataset)))\n",
        "    return test_loss"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ghXERz6psji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_test_plus_ks_init(n_epochs):\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "  \n",
        "        epoch_loss_valid = test_plus_ks_init(test_loader, model, cuda=True)\n",
        "        \n",
        "        epoch += 1"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt4uufeDvMRX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4751200e-4d7a-45e0-ec5e-931f2277ae1e"
      },
      "source": [
        "run_test_plus_ks_init(1)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 574/602 (95%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9bxggOnzNnc",
        "colab_type": "text"
      },
      "source": [
        "Точность не упала\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-4ISbAxifQu",
        "colab_type": "text"
      },
      "source": [
        "## 2. Тренируем аудио-генератор, который является \"атакующим\"\n",
        "### Подберем оптимальные параметры для алгоритма генерации музыки\n",
        "\n",
        "Реализуем защищённый градиентный спуск (projected gradient descent) с следующими ограничениями: (1e-3,1-1e-3). Данные значения были подобраны методом пробы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9g9qDRcGf7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define criterion \n",
        "criterion_att = CrossEntropyLoss()\n",
        "\n",
        "# Define model\n",
        "att_model = myAlexa(161) # 128 mel spec values for each window\n",
        "\n",
        "# Move model to cuda\n",
        "att_model.cuda()\n",
        "AG.cuda()\n",
        "\n",
        "# Define audio gen optimizer\n",
        "initial_current_params = [p.data.clone() for p in AG.parameters()]\n",
        "params = list(att_model.parameters()) + initial_current_params\n",
        "optimizer_att = torch.optim.SGD(params, lr=1e-3)\n",
        "\n",
        "# Define train function\n",
        "def train_audio_gen(loader, model, audio_gen, optimizer, criterion, epoch, cuda, log_interval, verbose=True):\n",
        "    model.eval()\n",
        "    audio_gen.train()\n",
        "\n",
        "    # Load basic generated wav\n",
        "    bach = spect_loader('/content/bach_audio11.wav', 0.02, 0.01, 'hann', True, max_len=101)  \n",
        "    \n",
        "    global_epoch_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "\n",
        "        song4_ = (('e', 6), ('f#', 6), ('e', 6), ('f#', 6), ('e', 6), ('f#', 6), ('e', 6), ('f#', 6))\n",
        "        audio_gen(song4_)\n",
        "        \n",
        "        bach = spect_loader('/content/bach_audio11.wav', 0.02, 0.01, 'hann', True, max_len=101)  \n",
        "        bach = bach.expand(batch_size,161,101)\n",
        "        data = torch.add(data, bach) # train sample + generated sample \n",
        "        \n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        try: \n",
        "            data = data.reshape(batch_size, -1, 161)\n",
        "            \n",
        "            # data, target = Variable(data), Variable(target)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)# .squeeze(1)\n",
        "            \n",
        "            loss = criterion(output.clamp(1e-3,1-1e-3), target.unsqueeze(-1))  # threshold the parameters and thus doing the projected gradient descent \n",
        "            loss = -loss  # flip sign   # THREAT MODEL\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            global_epoch_loss += loss.item()\n",
        "            if verbose:\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                            epoch, batch_idx * len(data), len(loader.dataset), 100.\n",
        "                            * batch_idx / len(loader), loss.item()))\n",
        "        except Exception:\n",
        "            print('E')\n",
        "            pass\n",
        "    return global_epoch_loss / len(loader.dataset)\n",
        "\n",
        "def test_audio_gen(loader, model, audio_gen, cuda, verbose=True):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    audio_gen.eval()\n",
        "    # Load generated wav\n",
        "    bach = spect_loader('/content/bach_audio11.wav', 0.02, 0.01, 'hann', True, max_len=101)  # THREAT MODEL\n",
        "    for data, target in loader:\n",
        "        bach = bach.expand(batch_size,161,101)\n",
        "        data = torch.add(data, bach) \n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        with torch.no_grad():\n",
        "            try: \n",
        "                data = data.reshape(batch_size, -1, 161)\n",
        "            \n",
        "                output = model(data)\n",
        "                test_loss += criterion(output , target.unsqueeze(-1)).item()\n",
        "                # test_loss += torch.nn.functional.nll_loss(output, target.unsqueeze(-1)).item()  # sum up batch loss\n",
        "                pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    test_loss /= len(loader.dataset)\n",
        "    if verbose:\n",
        "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(loader.dataset), 100. * correct / len(loader.dataset)))\n",
        "    return test_loss\n",
        "\n",
        "\n",
        "def run_train_audio_gen(n_epochs=1):\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_loss_train = train_audio_gen(train_loader, model=att_model, audio_gen=AG, optimizer=optimizer_ks, criterion=criterion_ks, epoch=epoch, cuda=True, log_interval=25, verbose=True)\n",
        "        epoch_loss_valid = test_audio_gen(test_loader, model=att_model, audio_gen=AG, cuda=True)\n",
        "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, epoch_loss_train))\n",
        "        epoch += 1"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnnfQTwDHLTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "529378bf-a14d-471d-9815-6d0fc11f22d7"
      },
      "source": [
        "run_train_audio_gen(n_epochs=1)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/2641 (0%)]\tLoss: -8.080856\n",
            "Train Epoch: 0 [800/2641 (30%)]\tLoss: -8.080856\n",
            "Train Epoch: 0 [1600/2641 (60%)]\tLoss: -8.080856\n",
            "Train Epoch: 0 [2400/2641 (90%)]\tLoss: -8.080856\n",
            "E\n",
            "\n",
            "Test set: Average loss: 0.2416, Accuracy: 19/602 (3%)\n",
            "\n",
            "Train Epoch: 0 \tLoss: -0.250901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW-hKOAfFENZ",
        "colab_type": "text"
      },
      "source": [
        "__Вывод :__ Точность сильно понизилась, значит параметры для атакующего аудио-генератора подобраны успешно "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2OO5uBTw79j",
        "colab_type": "text"
      },
      "source": [
        "### Генерируем струнную музыку `bach_audio2.wav` c помощью Карплус-Стронг алгоритма с оптимизированными для атаки параметрами  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ma6bo-xkeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AG = AudioGenKS(fn='/content/bach_audio2.wav')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N63dTaln0jwG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d2fe849b-3de9-404d-da7a-350a357eec49"
      },
      "source": [
        "song4_ = (\n",
        "  ('e', 6), ('f#', 6), ('e', 6), ('f#', 6), ('e', 6), ('f#', 6), ('e', 6), ('f#', 6))\n",
        "AG(song4_)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/8]\t\n",
            "[5/8]\t\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}